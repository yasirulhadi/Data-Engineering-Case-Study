# Data-Engineering-Case-Study

This repository contains a data engineering solution for AdvertiseX, a digital advertising technology company that manages multiple online advertising campaigns for its clients. The solution addresses the challenges of ingesting, processing, storing, and analyzing vast amounts of data generated by ad impressions, clicks, conversions, and bid requests.

## Architecture

The solution follows a distributed architecture with the following components:

1. **Data Ingestion**: Apache Kafka is used as a distributed messaging system to ingest data from various sources in real-time and batch modes.
2. **Data Processing**: Apache Spark Structured Streaming is employed for data processing, including data transformation, enrichment, deduplication, and correlation of ad impressions with clicks and conversions.
3. **Data Storage**: Processed data is stored in a data lake (e.g., Amazon S3, Azure Data Lake Storage, or Google Cloud Storage) in Parquet format for efficient querying and analytics.
4. **Data Warehouse**: A cloud-based data warehouse solution (e.g., Amazon Redshift, Azure Synapse Analytics, or Google BigQuery) is used for analytical workloads and fast querying.
5. **Error Handling and Monitoring**: Data quality checks, error handling mechanisms, and monitoring and alerting systems are implemented throughout the data pipeline.

## Requirements

- Python 3.7+
- Apache Spark 3.0+
- Apache Kafka 2.8+
- Cloud storage (e.g., Amazon S3, Azure Data Lake Storage, or Google Cloud Storage)
- Cloud data warehouse (e.g., Amazon Redshift, Azure Synapse Analytics, or Google BigQuery)
- Monitoring and alerting platform (e.g., AWS CloudWatch, Datadog, or Prometheus)

## Installation

1. Clone the repository:
2. Install the required Python packages:
3. Configure the necessary settings (e.g., Kafka broker addresses, cloud storage credentials, data warehouse connection details) in the `config.py` file.

## Usage

1. **Data Ingestion**: Run the data ingestion scripts to produce data to the Kafka topics (`ad_impressions`, `clicks`, `conversions`, and `bid_requests`). Example scripts are provided in the `ingestion` directory.

2. **Data Processing**: Start the Apache Spark Structured Streaming job by running the `streaming_job.py` script. This script will consume data from Kafka, perform transformations and correlations, and write the processed data to the data lake.

3. **Data Storage and Querying**: Schedule an ETL pipeline (e.g., using Apache Airflow or AWS Glue) to load data from the data lake to the data warehouse periodically. Example ETL scripts are provided in the `etl` directory.

4. **Error Handling and Monitoring**: Configure the monitoring and alerting platform to track pipeline health, performance, and data quality metrics. Set up alerting mechanisms to notify relevant stakeholders about data quality issues or pipeline failures.
